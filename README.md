# avocado

## Introduction

From [Kaggle](https://www.kaggle.com/neuromusic/avocado-prices):

> It is a well known fact that Millenials LOVE Avocado Toast. It's also a well known fact that all Millenials live in their parents basements.  Clearly, they aren't buying homes because they are buying too much Avocado Toast!
> 
> But maybe there's hopeâ€¦ if a Millenial could find a city with cheap avocados, they could live out the Millenial American Dream.

### About the Project

This is our final project for COE 332, Software Engineering Design at the University of Texas at Austin.

Through this project we aim to demonstrate our understanding of concurrency and asynchronous programming, REST APIs, container orchestration, and database operations.

Our system consists of 4 main components:

* **A web API** which accepts incoming requests and queues them for processing,
* **A redis database** which stores the queue and job information,
* **Scalable worker nodes** which concurrently execute jobs, and
* **A postgres database** which stores our avocado dataset.

You can find the course materials and project requirements [here](https://coe-332-sp21.readthedocs.io/en/main/homework/final_project.html).

### About the Dataset

This project is based on historical data provided by the Hass Avocado Board.  Our data and the introduction above are from Kaggle and can be accessed [here](https://www.kaggle.com/neuromusic/avocado-prices).

The data has the following format:

| Column Name | Description |
| ----------- | ----------- |
| id | the date of the observation
| week_id | the week of the year, 1-52
| week | the week of the observation
| price | the average price of a single avocado
| volume | the total number of avocados sold
| total_4046 | the total number of avocados with PLU 4046 sold
| total_4225 | the total number of avocados with PLU 4225 sold
| total_4770 | the total number of avocados with PLU 4770 sold
| category | conventional or organic
| year | the year of the observation
| region | the city or region of the observation

## Usage

Our system supports a variety of actions (jobs) which interact with and run analysis on our dataset.  All jobs must be submitted to our API where they are queued for processing by worker nodes.

The following job types are supported:

* Insert
* Query
* Update
* Delete
* Plot
* Summary

To learn how to define a job and for syntax examples, click [here](worker/README.md).

### Preferred Method: Web Application

Our web application can be accessed at https://isp-proxy.tacc.utexas.edu/phart/index.

### Alternate Method: Interact directly with our API

If you prefer to submit jobs in raw JSON format, you can do so using the `raw_jobs` route.  See `avocado/worker/README.md` for proper job formatting and examples.

Jobs can be sent via POST request using curl or a program like Postman.

#### Using Curl

Since the job structure can be complex, it is easiest to save the JSON to a file and use curl to POST from the file.

First, create a new file `job.json` and add the job details as described in `avocado/worker/README.md`:

```json
{
    "job_type": "query",
    "status": "submitted",
    "cols": ["id", "week", "volume", "price"],
    "params": [{
        "column": "year",
        "type": "equals",
        "value": 2018
        },
        {
        "column": "week_id",
        "type": "equals",
        "value": 3
        }]
}
```

Next, use CURL to send a POST request.

```
[avocado]$ curl -X POST -H "content-type: application/json" -d @data.json https://isp-proxy.tacc.utexas.edu/phart/raw_jobs
{
    "job_type": "query",
    "status": "submitted",
    "cols": [
        "id",
        "week",
        "volume",
        "price"
    ],
    "params": [
        {
            "column": "year",
            "type": "equals",
            "value": 2018
        },
        {
            "column": "week_id",
            "type": "equals",
            "value": 3
        }
    ],
    "id": "3479bc65-1484-4316-8efa-de33e63ea961",
    "submitted": "2021-05-05 20:04:17.336567"
}
```

To check on a job's status, or to access a completed job, you can use the `get_job` route.

```
[avocado]$ curl https://isp-proxy.tacc.utexas.edu/phart/get_job/<jobid>
<your job here>
```

To access the image generated by a plot job, you will need the job id assigned when your job was submitted.  You can download the image using `wget`.

```bash
[avocado]$ wget https://isp-proxy.tacc.utexas.edu/phart/download/<jobid>
```

## Deployment Instructions

### Production Deployment Instructions (Kubernetes)

Section in progress...

### Test Deployment Instructions (docker-compose)

For rapid testing and development, the project can be launched using docker-compose.

Use the `-d` flag to run in the daemon mode and the `--build` flag to rebuild the containers on launch (optional).

```
docker-compose up -d --build
...
Starting avocado_redis_1    ... done
Starting avocado_postgres_1 ... done
Starting avocado_api_1      ... done
Starting avocado_worker_1   ... done
```

To launch multiple workers, add `--scale worker={num_workers}`

```
docker-compose up -d --scale worker=3
Starting avocado_redis_1    ... done
Starting avocado_postgres_1 ... done
Starting avocado_api_1      ... done
Starting avocado_worker_1   ... done
Starting avocado_worker_2   ... done
...
```
